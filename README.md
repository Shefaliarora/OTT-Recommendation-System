# OTT-Recommendation-System
ğŸš€ Built a Netflix-style Personalized Content Recommendation Engine using:

âœ… Real-time user event ingestion â†’ Kafka

 âœ… User-item interaction store â†’ DynamoDB
 
 âœ… Model training pipeline â†’ CSV â†’ S3 â†’ Amazon Personalize
 
 âœ… Trained model deployed as live API â†’ serving recommendations
 
 âœ… Tested with Python â†’ API returns top movie recommendations for each user.

Include the architecture diagram we created:

Kafka â†’ DynamoDB â†’ S3 â†’ Personalize â†’ Model â†’ Campaign â†’ API â†’ React

Skills demonstrated:
âœ… AWS Personalize
 âœ… Kafka Streaming
 âœ… Data Engineering Pipeline â†’ DynamoDB â†’ S3 â†’ Personalize
 âœ… Real-world RecSys architecture
 âœ… Model training â†’ API deployment â†’ testing

Components
Kafka Producer
Simulate ott-view-events (userId, itemId, eventType, timestamp)

Kafka Consumer
Store interactions into DynamoDB table UserItemInteractions

DynamoDB â†’ CSV Export
Export interactions â†’ CSV â†’ ready for Personalize import

## ğŸš€ Steps â€” OTT Content Recommendation System Demo

**Step 1 â€” Kafka Topic:** `ott-view-events` â†’ âœ… *(created Kafka topic to capture user view events)*

**Step 2 â€” Kafka Producer:** `producer-code.py` â†’ âœ… *(simulated user watch events â†’ sent to Kafka topic `ott-view-events`)*

**Step 3 â€” Kafka Consumer:** `consumer-dynamodb.py` â†’ âœ… *(listened on topic â†’ processed events â†’ stored user-item interactions in DynamoDB table `UserItemInteractions`)*

**Step 4 â€” DynamoDB Table:** `UserItemInteractions` â†’ âœ… *(created table with `userId`, `itemId`, `eventType`, `watchCount`, `lastWatchedTimestamp`)*

**Step 5 â€” DynamoDB Export to CSV:** `newdynamoapi.py` or `personalize-interactions.py` â†’ âœ… *(exported DynamoDB interactions â†’ CSV `personalize-interactions.csv` â†’ required format for Personalize)*

**Step 6 â€” Upload CSV to S3:** â†’ âœ… *(uploaded CSV to `s3://ott-demo-bucketjune7/personalize/interactions/`)*

**Step 7 â€” Personalize Dataset Group:** `OTT-Recommendation-Group` â†’ âœ… *(created Dataset Group in Amazon Personalize)*

**Step 8 â€” Personalize Dataset:** `OTT-Interactions` â†’ âœ… *(created Interactions Dataset with required schema â†’ `user_id`, `item_id`, `event_type`, `timestamp`)*

**Step 9 â€” Dataset Import Job:** â†’ âœ… *(imported CSV from S3 â†’ Personalize Dataset â†’ Import job completed successfully)*

**Step 10 â€” Solution:** `solution-OTT-June9` â†’ âœ… *(created Solution using `aws-user-personalization` recipe)*

**Step 11 â€” Solution Version:** â†’ âœ… *(created Solution Version â†’ triggered model training â†’ Solution Version ACTIVE)*

**Step 12 â€” Campaign:** `OTT-Recommendation-Campaign` â†’ âœ… *(deployed Campaign â†’ live API endpoint created â†’ ACTIVE)*

**Step 13 â€” Python API Test:** `personalize_get_recommendations.py` â†’ âœ… *(tested Personalize API â†’ retrieved personalized movie recommendations for userId â†’ working successfully)*



Amazon Personalize

âœ… Dataset Group â†’ OTT-Recommendation-Group
âœ… Interactions Dataset â†’ user_id, item_id, event_type, timestamp
âœ… Solution â†’ aws-user-personalization recipe
âœ… Solution Version â†’ trained model
âœ… Campaign â†’ live recommendations API

Python API Testing
Use Personalize get_recommendations(userId) API â†’ test recommendations

ğŸ› ï¸ Setup Steps

1ï¸âƒ£ Simulate Kafka â†’ ingest events

2ï¸âƒ£ Store interactions in DynamoDB

3ï¸âƒ£ Export CSV â†’ S3

4ï¸âƒ£ Import dataset into Personalize

5ï¸âƒ£ Train Solution â†’ Create Solution Version

6ï¸âƒ£ Deploy Campaign â†’ API ready

7ï¸âƒ£ Test recommendations using Python

File	Purpose
producer-code.py	Generate real-time user view events â†’ Kafka

consumer-dynamodb.py	Store events in DynamoDB â†’ UserItemInteractions

newdynamoapi.py	Export DynamoDB â†’ CSV â†’ Personalize format

generate_interactions.py	Generate synthetic interactions for Personalize

personalize_get_recommendations.py	Test API â†’ Get movie recommendations

Example Output

Recommendations for user1:
ItemId: movie23
ItemId: movie17
ItemId: movie45
